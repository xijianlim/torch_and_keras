{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appropriated/Inspired from https://github.com/hunkim/PyTorchZeroToAll/blob/master/13_1_rnn_classification_basics.py\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pygsheets\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#note: training data removed due to company policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>make_model</th>\n",
       "      <th>token_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>bmw 2 series coupe/conv</td>\n",
       "      <td>('bmw', '2-series')</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>bmw 2-series coupe/conv</td>\n",
       "      <td>('bmw', '2-series')</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>bmw 3</td>\n",
       "      <td>('bmw', '3-series')</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>bmw 3 series</td>\n",
       "      <td>('bmw', '3-series')</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>bmw 3 series coupe/conv</td>\n",
       "      <td>('bmw', '3-series')</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>bmw 3 series gran turismo</td>\n",
       "      <td>('bmw', '3-series')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>bmw 330 gt</td>\n",
       "      <td>('bmw', '3-series')</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>bmw 4</td>\n",
       "      <td>('bmw', '4-series')</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>bmw 4 series coupe/conv</td>\n",
       "      <td>('bmw', '4-series')</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>bmw 4 series gran coupe</td>\n",
       "      <td>('bmw', '4-series')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Database           make_model  token_size\n",
       "120    bmw 2 series coupe/conv  ('bmw', '2-series')           4\n",
       "121    bmw 2-series coupe/conv  ('bmw', '2-series')           3\n",
       "122                      bmw 3  ('bmw', '3-series')           2\n",
       "123               bmw 3 series  ('bmw', '3-series')           3\n",
       "124    bmw 3 series coupe/conv  ('bmw', '3-series')           4\n",
       "125  bmw 3 series gran turismo  ('bmw', '3-series')           5\n",
       "126                 bmw 330 gt  ('bmw', '3-series')           3\n",
       "127                      bmw 4  ('bmw', '4-series')           2\n",
       "128    bmw 4 series coupe/conv  ('bmw', '4-series')           4\n",
       "129    bmw 4 series gran coupe  ('bmw', '4-series')           5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[120:130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating your own torch dataset\n",
    "#https://github.com/hunkim/PyTorchZeroToAll/blob/master/13_2_rnn_classification.py\n",
    "##https://blog.godatadriven.com/fairness-in-pytorch\n",
    "\n",
    "class NameDataset(Dataset):\n",
    "\n",
    "\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self,df, is_train_set=False):\n",
    "        \n",
    "\n",
    "        self.text_input = list(df['Database'])\n",
    "        self.make_model = list(df['make_model'])\n",
    "        self.len = len(self.make_model)\n",
    "\n",
    "        self.make_model_list = list(sorted(set(self.make_model)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.text_input[index], self.make_model[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def get_make_models(self):\n",
    "        return self.make_model_list\n",
    "\n",
    "    def get_make_model(self, id):\n",
    "        return self.make_model_list[id]\n",
    "\n",
    "    def get_make_model_id(self, make_model):\n",
    "        return self.make_model_list.index(make_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=256\n",
    "\n",
    "# Using PyTorch's DataLoader\n",
    "train_dataset = NameDataset(df_train,is_train_set=True)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = NameDataset(df_train,is_train_set=False)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "data=NameDataset(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('', '')\", \"('acura', 'cdx')\", \"('acura', 'mdx')\", \"('acura', 'rdx')\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.make_model_list[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"('acura', 'rdx')\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.get_make_model(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### converting characters to numercals\n",
    "\n",
    "def str2ascii_arr(msg):\n",
    "    arr = [ord(c) for c in msg]\n",
    "    return arr, len(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([118, 111, 108, 107, 115, 119, 97, 103, 101, 110, 32, 97, 109, 97, 114, 111, 107], 17)\n"
     ]
    }
   ],
   "source": [
    "print(str2ascii_arr(u'volkswagen amarok'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting the Make Model index IDs when search on make models\n",
    "\n",
    "def models2tensor(models,dataset):\n",
    "    model_ids = [dataset.get_make_model_id(\n",
    "        model) for model in models]\n",
    "    return torch.LongTensor(model_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of the Models in the train dataset: [\"('', '')\", \"('acura', 'cdx')\", \"('acura', 'mdx')\", \"('acura', 'rdx')\", \"('acura', 'tlx')\"]\n",
      "tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "model_sample=data.make_model_list[:5]\n",
    "model_ids=models2tensor(model_sample,train_dataset)\n",
    "print(\"Sample of the Models in the train dataset: {a}\".format(a=model_sample))\n",
    "print(model_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Padding and making variables\n",
    "\n",
    "def create_variable(tensor):\n",
    "    # Do cuda() before wrapping with variable\n",
    "    if torch.cuda.is_available():\n",
    "        return Variable(tensor.cuda())\n",
    "    else:\n",
    "        return Variable(tensor)\n",
    "\n",
    "\n",
    "def pad_sequences(vectorized_seqs, seq_lengths, models):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "\n",
    "    # Sort tensors by their length\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "    seq_tensor = seq_tensor[perm_idx]\n",
    "\n",
    "    # Also sort the target (countries) in the same order\n",
    "    target = models2tensor(models, train_dataset)\n",
    "    if len(models):\n",
    "        target = target[perm_idx]\n",
    "\n",
    "    # Return variables\n",
    "    # DataParallel requires everything to be a Variable\n",
    "    return create_variable(seq_tensor), \\\n",
    "        create_variable(seq_lengths), \\\n",
    "        create_variable(target)\n",
    "\n",
    "def make_variables(names, countries):\n",
    "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
    "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
    "    return pad_sequences(vectorized_seqs, seq_lengths, countries)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (text, models) in enumerate(train_loader, 1):\n",
    "        input, seq_lengths, target= make_variables(text, models)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "256\n",
      "text_input:  ('ferrari fxx-k',)\n",
      "make and model  ('ferrari', 'fxx-k')\n",
      "tensored example [102, 101, 114, 114, 97, 114, 105, 32, 102, 120, 120, 45, 107]\n",
      "\n",
      "example of padded input:  tensor([100, 115,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0])\n",
      "tensor([[102, 101, 114, 114,  97, 114, 105,  32, 102, 120, 120,  45, 107]]) tensor([13])\n"
     ]
    }
   ],
   "source": [
    "### This is an example \n",
    "\n",
    "for i, (text, models) in enumerate(test_loader, 1):\n",
    "    if i ==1:\n",
    "        print(i)\n",
    "        text_x=text[0], \n",
    "        model_x=models[0]\n",
    "        print(len(models))\n",
    "        print('text_input: ',text_x)\n",
    "        print('make and model ', model_x)\n",
    "        print('tensored example',str2ascii_arr(text[0])[0])\n",
    "        print('')\n",
    "        print('example of padded input: ',input[-1])\n",
    "        input_x, seq_lengths_x, target_x = make_variables([(text_x)[0]], [])\n",
    "        print(input_x, seq_lengths_x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "bugatti veyron\n",
      "('bugatti', 'veyron')\n",
      "tensored example [98, 117, 103, 97, 116, 116, 105, 32, 118, 101, 121, 114, 111, 110]\n",
      "tensor([[ 98, 117, 103,  97, 116, 116, 105,  32, 118, 101, 121, 114, 111, 110]]) tensor([121])\n"
     ]
    }
   ],
   "source": [
    "for i, (text, models) in enumerate(train_loader, 1):\n",
    "    if i ==1:\n",
    "        print(i)\n",
    "        print(text[0])\n",
    "        print(models[0])\n",
    "        print('tensored example',str2ascii_arr(text[0])[0])\n",
    "        input_x, seq_lengths_x, target_x = make_variables([(text)[0]], (models))\n",
    "        print(input_x, target_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    # Our model\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = int(bidirectional) + 1\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, seq_lengths):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        # input shape: B x S (input size)\n",
    "        # transpose to make S(sequence) x B (batch)\n",
    "        input = input.t()\n",
    "        batch_size = input.size(1)\n",
    "\n",
    "        # Make a hidden\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "\n",
    "        # Embedding S x B -> S x B x I (embedding size)\n",
    "        embedded = self.embedding(input)\n",
    "\n",
    "        # Pack them up nicely\n",
    "        gru_input = pack_padded_sequence(\n",
    "            embedded, seq_lengths.data.cpu().numpy())\n",
    "        \n",
    "        ''' pack_padded_sequence package\n",
    "        They are used for seq to seq models with variable lengths. \n",
    "        Such as a sentence can be of variable length, \n",
    "        and to feed it into any class of an RNN, you need to be able to get the output at the \n",
    "        right time step.\n",
    "        '''\n",
    "        # To compact weights again call flatten_parameters().\n",
    "        self.gru.flatten_parameters()\n",
    "        output, hidden = self.gru(gru_input, hidden)\n",
    "\n",
    "        # Use the last layer output as FC's input\n",
    "        # No need to unpack, since we are going to use hidden\n",
    "        fc_output = self.fc(hidden[-1])\n",
    "        return fc_output\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers * self.n_directions,\n",
    "                             batch_size, self.hidden_size)\n",
    "        \n",
    "        ## note: the initial hidden size is determined by the batch og the data and the \n",
    "        ## architechture of the model\n",
    "        return hidden\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def time_since(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "HIDDEN_SIZE=256\n",
    "N_LAYERS=3\n",
    "N_MODELS=len(data.make_model_list)\n",
    "N_CHARS = 128  # ASCII\n",
    "\n",
    "\n",
    "num_epochs=40\n",
    "total_loss = 0\n",
    "iter = 0\n",
    "learning_rate=1e-3\n",
    "\n",
    "model = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_MODELS, N_LAYERS)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1m 37s] Train Epoch: 7 [256/1645 (16%)] loss:3.38, total_loss: 0.97 iter:50\n",
      "\n",
      "Test set: Accuracy: 498/1645 (30%)\n",
      "\n",
      "[3m 18s] Train Epoch: 14 [256/1645 (16%)] loss:1.81, total_loss: 1.46 iter:100\n",
      "\n",
      "Test set: Accuracy: 1057/1645 (64%)\n",
      "\n",
      "[5m 3s] Train Epoch: 21 [256/1645 (16%)] loss:0.86, total_loss: 1.70 iter:150\n",
      "\n",
      "Test set: Accuracy: 1459/1645 (88%)\n",
      "\n",
      "[6m 47s] Train Epoch: 28 [256/1645 (16%)] loss:0.46, total_loss: 1.82 iter:200\n",
      "\n",
      "Test set: Accuracy: 1601/1645 (97%)\n",
      "\n",
      "[8m 34s] Train Epoch: 35 [256/1645 (16%)] loss:0.22, total_loss: 1.88 iter:250\n",
      "\n",
      "Test set: Accuracy: 1632/1645 (99%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iter_n=[]\n",
    "loss_n=[]\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (names, countries) in enumerate(train_loader):\n",
    "        input, seq_lengths, target = make_variables(names, countries)\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #make the class\n",
    "        output = model(input, seq_lengths)\n",
    "        \n",
    "        #calculate loss\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 50 == 0:\n",
    "            print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)] loss:{:.2f}, total_loss: {:.2f} iter:{}'.format(\n",
    "                time_since(start), \n",
    "                epoch,  \n",
    "                len(names), \n",
    "                len(train_loader.dataset),\n",
    "                (len(names) / len(train_loader.dataset))*100,\n",
    "                loss,\n",
    "                total_loss / len(names), \n",
    "                iter)\n",
    "                 )\n",
    "            \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            iter_n.append(iter)\n",
    "            loss_n.append(loss)\n",
    "            \n",
    "            for i, (names, countries) in enumerate(test_loader):\n",
    "                input, seq_lengths, target = make_variables(names, countries)\n",
    "                output = model(input, seq_lengths)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "                total += len(countries)\n",
    "            \n",
    "            print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            correct, total, 100. * correct / total))\n",
    "            \n",
    "            \n",
    "            #loss_n.append(total_loss)\n",
    "            i#ter_n.append(iter)\n",
    "            \n",
    "            \n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Training Loss')\n",
    "plt.plot(iter_n,loss_n, '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volks tigy is ('volkswagen', 'tiguan') input:  tensor([[118, 111, 108, 107, 115,  32, 116, 105, 103, 121]])\n",
      "mercedes benz c240 is ('mercedes-benz', 'c-class') input:  tensor([[109, 101, 114,  99, 101, 100, 101, 115,  32,  98, 101, 110, 122,  32,\n",
      "          99,  50,  52,  48]])\n",
      "merc glc is ('mercedes-benz', 'glc-class') input:  tensor([[109, 101, 114,  99,  32, 103, 108,  99]])\n",
      "citron ds3 is ('citroen', 'ds4') input:  tensor([[ 99, 105, 116, 114, 111, 110,  32, 100, 115,  51]])\n",
      "lexus rx300 is ('lexus', 'rx') input:  tensor([[108, 101, 120, 117, 115,  32, 114, 120,  51,  48,  48]])\n",
      "mitubishi mirage is ('mitsubishi', 'mirage') input:  tensor([[109, 105, 116, 117,  98, 105, 115, 104, 105,  32, 109, 105, 114,  97,\n",
      "         103, 101]])\n",
      "mg 3 is ('mg', 3) input:  tensor([[109, 103,  32,  51]])\n",
      "saab 93 is ('saab', '9-3') input:  tensor([[115,  97,  97,  98,  32,  57,  51]])\n",
      "bmw x3 is ('bmw', 'x3') input:  tensor([[ 98, 109, 119,  32, 120,  51]])\n",
      "land rover range velar is ('land rover', 'range rover velar') input:  tensor([[108,  97, 110, 100,  32, 114, 111, 118, 101, 114,  32, 114,  97, 110,\n",
      "         103, 101,  32, 118, 101, 108,  97, 114]])\n",
      "merc c300 is ('mercedes-benz', 'c-class') input:  tensor([[109, 101, 114,  99,  32,  99,  51,  48,  48]])\n",
      "merkedez s-ceras is ('mercedes-benz', 'r-class') input:  tensor([[109, 101, 114, 107, 101, 100, 101, 122,  32, 115,  45,  99, 101, 114,\n",
      "          97, 115]])\n",
      "miny cooper is ('mini', 'cooper') input:  tensor([[109, 105, 110, 121,  32,  99, 111, 111, 112, 101, 114]])\n"
     ]
    }
   ],
   "source": [
    "def test(name=None):\n",
    "    # Predict for a given name\n",
    "    if name:\n",
    "        input, seq_lengths, target = make_variables([name], [])\n",
    "        output = model(input, seq_lengths)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        model_id = pred.cpu().numpy()[0][0]\n",
    "        print(\"input: \", name, \" --- predicted \", train_dataset.get_make_model(model_id)\n",
    "        return\n",
    "    \n",
    "test('volks tigy')\n",
    "test('mercedes benz c240')\n",
    "test('merc glc')\n",
    "test('citron ds3')\n",
    "test('lexus rx300')\n",
    "test('mitubishi mirage')\n",
    "test('mg 3')\n",
    "test('saab 93')\n",
    "test('bmw x3')\n",
    "test('land rover range velar')\n",
    "test('merc c300')\n",
    "test('merkedez s-ceras')\n",
    "test('miny cooper')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving model and dataset\n",
    "\n",
    "save_model = False\n",
    "if save_model is True:\n",
    "    # Saves only parameters\n",
    "    # alpha & beta\n",
    "    torch.save(model.state_dict(), \"translation_model.pth\")\n",
    "# torch.save(model.state_dict(), \"translation_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"translation_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('make_model_vocab.pkl', 'wb') as output:\n",
    "    pickle.dump(train_dataset, output, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('trained_model.pkl', 'wb') as output:\n",
    "    pickle.dump(RNNClassifier, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
