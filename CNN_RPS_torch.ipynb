{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "basedir=os.path.abspath(os.path.dirname('__file__'))\n",
    "data_folder=os.path.join(basedir, 'data') # this is folder where the images are saved\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 28\n",
    "n_iters = 3000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from https://www.reddit.com/r/learnmachinelearning/comments/92nh4c/how_do_i_load_images_into_pytorch_for_training/\n",
    "#images courtesy of Kaggle at - https://www.kaggle.com/drgfreeman/rockpaperscissors\n",
    "\n",
    "\n",
    "def load_images(image_size=28, batch_size=batch_size, root=data_folder):\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Resize(image_size),\n",
    "                    transforms.CenterCrop(image_size),\n",
    "                    transforms.Grayscale(num_output_channels=3),\n",
    "                    transforms.ToTensor()\n",
    "#                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "        \n",
    "    ])\n",
    "\n",
    "    train_set = datasets.ImageFolder(root=root, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    test_set= datasets.ImageFolder(root=root, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    return train_set, train_loader, test_set, test_loader    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_set, train_loader, test_set, test_loader  =load_images(image_size=28, batch_size=batch_size, root=data_folder)\n",
    "\n",
    "num_epochs = n_iters / (len(train_set) / batch_size)\n",
    "num_epochs = int(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### a sample of the image ###\n",
    "\n",
    "plt.imshow(train_set[0][0].permute(1, 2, 0)  )\n",
    "print(train_set[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper': 0, 'rock': 1, 'scissors': 2}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0][0].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculating padding\n",
    "height=28\n",
    "width = 28\n",
    "kernel_size = 5 # we\n",
    "padding = int((kernel_size-1)/2)\n",
    "stride=1\n",
    "\n",
    "#output_after_1_convolution\n",
    "out1=(height-kernel_size+(2*padding))/1 + 1 #=28\n",
    "\n",
    "output_pool1 = out1/padding #=14\n",
    "\n",
    "\n",
    "out2=(output_pool1-kernel_size+(2*padding))/1 + 1 #= 14\n",
    "\n",
    "output_pool2 = out2/padding #=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.0\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "print(output_pool1)\n",
    "print(output_pool2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kernel_size-1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32 * output_pool2 * output_pool2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "    \n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=kernel_size, stride=1, padding=padding)\n",
    "        self.relu1 = nn.ReLU() \n",
    "        #inchannel = 1 because it is one grey scale #out_channels=number of kernels you're choosing ,so 16 feature maps\n",
    "        \n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2) #maxpool kernel = (kernel_size-1)/2 = (5-1)/2 = 2\n",
    "     \n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected 1 (readout)\n",
    "        self.fc1 = nn.Linear(int(32 * output_pool2 * output_pool2), 3) ## so out_channels * output pools, and 3 represents the number of labels (rock paper or scissors)\n",
    "        \n",
    "        # 32*7*7 = final conv output X output_pool2 X output_pool2\n",
    "        # '10' refers to the labels len(torch.unique(test_dataset.test_labels)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "        \n",
    "        # Convolution 2 \n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # Max pool 2 \n",
    "        out = self.maxpool2(out)\n",
    "        \n",
    "        # Resize\n",
    "        # Original size: (100, 32, 7, 7)\n",
    "        # out.size(0): 100\n",
    "        # New out size: (100, 32*7*7)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel()\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x1241669a8>\n",
      "length of parameters : 6\n",
      "\n",
      " first outout, first input , kernel dimensions :\n",
      "torch.Size([16, 3, 5, 5])\n",
      "torch.Size([16])\n",
      "\n",
      " second outout, second input , kernel dimensions :\n",
      "torch.Size([32, 16, 5, 5])\n",
      "torch.Size([32])\n",
      "\n",
      "number of labels=3, 1569= 32*7*7 as per max pooling calcs above\n",
      "torch.Size([3, 1568])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "\n",
    "print('length of parameters :',len(list(model.parameters())))\n",
    "\n",
    "# Convolution 1: 16 Kernels\n",
    "print(\"\\n first outout, first input , kernel dimensions :\")\n",
    "print(list(model.parameters())[0].size())\n",
    "\n",
    "# Convolution 1 Bias: 16 Kernels\n",
    "print(list(model.parameters())[1].size())\n",
    "\n",
    "print(\"\\n second outout, second input , kernel dimensions :\")\n",
    "# Convolution 2: 32 Kernels with depth = 16\n",
    "print(list(model.parameters())[2].size())\n",
    "\n",
    "# Convolution 2 Bias: 32 Kernels with depth = 16\n",
    "\n",
    "print(list(model.parameters())[3].size())\n",
    "\n",
    "# Fully Connected Layer 1\n",
    "\n",
    "print('\\nnumber of labels=3, 1569= 32*7*7 as per max pooling calcs above')\n",
    "print(list(model.parameters())[4].size())\n",
    "\n",
    "# Fully Connected Layer Bias\n",
    "print(list(model.parameters())[5].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7c943c218c3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Forward pass to get output/logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Calculate Loss: softmax --> cross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/caradvice_ml/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-ef14c0aea322>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Convolution 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/caradvice_ml/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/caradvice_ml/venv/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 338\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images\n",
    "        images = images.requires_grad_()\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images\n",
    "                images = images.requires_grad_()\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing One Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing from a Single Image taken from the dataset\n",
    "\n",
    "\n",
    "test_image=test_set[0][0]\n",
    "printing_image=test_image\n",
    "test_image = Variable(test_image, requires_grad=True)\n",
    "test_image = test_image.unsqueeze(0)\n",
    "outputs=model(test_image)\n",
    "print(outputs[0,:])\n",
    "print('\\n predicted Label is :')\n",
    "print(torch.max(outputs.data, 1)[1][0])\n",
    "pred_label=int(torch.max(outputs.data, 1)[1][0])\n",
    "\n",
    "if pred_label == 0:\n",
    "    print(\"predicted label is Paper\")\n",
    "elif pred_label == 1:\n",
    "    print(\"predicted label is Rock\")\n",
    "else:\n",
    "    print(\"predicted label is Scissors\")\n",
    "\n",
    "print('\\n correct label is ')\n",
    "print(test_set[0][1])\n",
    "\n",
    "actual_label=test_set[0][1]\n",
    "\n",
    "if actual_label == 0:\n",
    "    print(\"actual label is Paper\")\n",
    "elif actual_label == 1:\n",
    "    print(\"actual label is Rock\")\n",
    "else:\n",
    "    print(\"actual label is Scissors\")\n",
    "\n",
    "\n",
    "\n",
    "if (torch.max(outputs.data, 1)[1][0])==(test_set[0][1]):\n",
    "    print(\"\\n correct Pred\")\n",
    "else:\n",
    "    print(\"better luck nekk time\")\n",
    "    \n",
    "    \n",
    "plt.imshow(printing_image.detach().permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving model and dataset\n",
    "\n",
    "save_model = False\n",
    "if save_model is True:\n",
    "    # Saves only parameters\n",
    "    # alpha & beta\n",
    "    torch.save(model.state_dict(), \"RPSclass.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caradvice_ml",
   "language": "python",
   "name": "caradvice_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
